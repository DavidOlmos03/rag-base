# Conceptos Clave de RAG (Retrieval-Augmented Generation)

Esta gu√≠a explica los conceptos fundamentales que debes entender para trabajar con el sistema RAG Base.

---

## üìÑ Chunking (Fragmentaci√≥n de Documentos)

### ¬øQu√© es?

El **chunking** es el proceso de dividir documentos largos en fragmentos (chunks) m√°s peque√±os y manejables. Esto es esencial porque:

- Los modelos de embeddings tienen l√≠mites de longitud de entrada
- Los LLMs tienen l√≠mites de contexto
- Fragmentos m√°s peque√±os permiten b√∫squedas m√°s precisas

### ¬øC√≥mo funciona en este proyecto?

Ubicaci√≥n: `app/rag/ingestion/chunker.py`

**Configuraci√≥n actual:**
- **Tama√±o del chunk**: 500 caracteres (configurable en `.env` como `DEFAULT_CHUNK_SIZE`)
- **Overlap**: 50 caracteres (configurable como `DEFAULT_CHUNK_OVERLAP`)

### Par√°metros importantes

```python
DEFAULT_CHUNK_SIZE=500        # Tama√±o de cada fragmento
DEFAULT_CHUNK_OVERLAP=50      # Superposici√≥n entre fragmentos
```

#### ¬øPor qu√© overlap?

El overlap (superposici√≥n) asegura que informaci√≥n importante en los l√≠mites entre chunks no se pierda:

```
Chunk 1: [0-500 caracteres]
                    ‚Üì Overlap (50 chars)
Chunk 2:        [450-950 caracteres]
                    ‚Üì Overlap (50 chars)
Chunk 3:            [900-1400 caracteres]
```

### Recomendaciones de configuraci√≥n

| Tipo de Documento | Chunk Size | Overlap | Raz√≥n |
|-------------------|------------|---------|-------|
| Documentos t√©cnicos | 400-600 | 50-100 | Balance entre contexto y precisi√≥n |
| Art√≠culos/Blogs | 600-800 | 100-150 | P√°rrafos completos |
| C√≥digo fuente | 200-400 | 30-50 | Funciones/bloques completos |
| FAQs | 100-200 | 20-30 | Preguntas/respuestas individuales |

### Metadata de chunks

Cada chunk incluye metadata √∫til:

```python
{
    "content": "Texto del fragmento...",
    "chunk_index": 0,
    "document_id": "uuid-del-documento",
    "filename": "documento.pdf",
    "file_type": "pdf",
    "metadata": {...}  # Metadata adicional
}
```

---

## üß† Embeddings (Representaciones Vectoriales)

### ¬øQu√© son?

Los **embeddings** son representaciones num√©ricas (vectores) de texto que capturan su significado sem√°ntico. Textos con significados similares tienen vectores similares.

```
"¬øC√≥mo funciona el motor?"     ‚Üí [0.2, 0.8, 0.1, ..., 0.5]  (768 dimensiones)
"¬øCu√°l es el funcionamiento?"  ‚Üí [0.25, 0.75, 0.15, ..., 0.48]  (similar)
"El gato est√° durmiendo"       ‚Üí [0.9, 0.1, 0.7, ..., 0.2]  (diferente)
```

### Modelo utilizado: BAAI/bge-m3

**Caracter√≠sticas:**
- **Dimensiones**: 1024 (m√°s dimensiones = mayor precisi√≥n)
- **Multiling√ºe**: Soporta espa√±ol, ingl√©s y 100+ idiomas
- **Max length**: 8192 tokens (configurable a 512 en este proyecto)
- **Tipo**: Dense embeddings

**Configuraci√≥n:**

```bash
EMBEDDING_MODEL=BAAI/bge-m3
EMBEDDING_DEVICE=cpu              # o 'cuda' para GPU
EMBEDDING_BATCH_SIZE=32           # Textos procesados por lote
EMBEDDING_MAX_LENGTH=512          # Longitud m√°xima de entrada
EMBEDDING_CACHE_TTL=2592000       # Cache: 30 d√≠as
```

### Proceso de generaci√≥n

```
1. Texto original ‚Üí "¬øCu√°l es la pol√≠tica de reembolso?"
2. Tokenizaci√≥n ‚Üí [2534, 8901, 3421, ...]
3. Modelo BAAI/bge-m3 ‚Üí Procesamiento
4. Vector de embeddings ‚Üí [0.234, -0.567, 0.891, ..., 0.123] (1024 dims)
5. Almacenamiento en Qdrant
```

### Cache de embeddings

**Ubicaci√≥n**: Redis

El sistema cachea embeddings para:
- **Mejorar rendimiento**: Evita recalcular el mismo texto
- **Reducir costos**: Menos procesamiento
- **TTL**: 30 d√≠as por defecto

```python
# La misma pregunta en diferentes queries usa el embedding cacheado
Query 1: "¬øCu√°l es el precio?" ‚Üí Calcula embedding ‚Üí Guarda en cache
Query 2: "¬øCu√°l es el precio?" ‚Üí Usa embedding del cache (instant√°neo)
```

---

## üîç Similarity Search (B√∫squeda por Similitud)

### ¬øC√≥mo funciona?

La b√∫squeda por similitud compara el embedding de tu query con los embeddings de todos los chunks almacenados:

```
Query: "¬øC√≥mo instalar la aplicaci√≥n?"
  ‚Üì Generar embedding
[0.3, 0.7, 0.2, ..., 0.5]
  ‚Üì Comparar con chunks en Qdrant

Chunk 1: "Instalaci√≥n paso a paso..." ‚Üí Similitud: 0.92 ‚úÖ
Chunk 2: "Proceso de instalaci√≥n..."  ‚Üí Similitud: 0.88 ‚úÖ
Chunk 3: "Pol√≠tica de privacidad..."  ‚Üí Similitud: 0.23 ‚ùå
Chunk 4: "Contacto y soporte..."      ‚Üí Similitud: 0.15 ‚ùå
```

### M√©trica de distancia: Cosine Similarity

Este proyecto usa **similitud coseno** (cosine similarity), que mide el √°ngulo entre vectores:

```
Score = cos(Œ∏) = (A ¬∑ B) / (||A|| ||B||)

Rango: -1 a 1
- 1.0  = Id√©nticos
- 0.7  = Muy similares (threshold por defecto)
- 0.5  = Moderadamente similares
- 0.0  = No relacionados
- -1.0 = Opuestos
```

**Ventajas de cosine**:
- Insensible a la magnitud del vector
- Ideal para similitud sem√°ntica
- Rango normalizado [0, 1] cuando se usa con vectores positivos

### Score Threshold (Umbral de Similitud)

El **score_threshold** filtra resultados poco relevantes:

```python
# En el endpoint query
{
    "query": "¬øC√≥mo resetear mi contrase√±a?",
    "score_threshold": 0.7  # Solo chunks con ‚â•70% similitud
}
```

#### Recomendaciones de threshold

| Threshold | Uso | Resultado |
|-----------|-----|-----------|
| **0.9 - 1.0** | B√∫squedas exactas | Muy pocos resultados, alta precisi√≥n |
| **0.7 - 0.9** | **Uso general** ‚úÖ | Balance entre precisi√≥n y recall |
| **0.5 - 0.7** | B√∫squedas amplias | M√°s resultados, menor precisi√≥n |
| **< 0.5** | Exploraci√≥n | Muchos resultados irrelevantes ‚ö†Ô∏è |

**Ejemplo pr√°ctico:**

```python
# Threshold = 0.9 (Estricto)
Query: "pol√≠tica de reembolso"
Resultados: 2 chunks (muy relevantes)

# Threshold = 0.7 (Balanceado) ‚úÖ
Query: "pol√≠tica de reembolso"
Resultados: 5 chunks (relevantes)

# Threshold = 0.5 (Permisivo)
Query: "pol√≠tica de reembolso"
Resultados: 15 chunks (incluye informaci√≥n tangencial)
```

### Top K (N√∫mero de resultados)

Controla cu√°ntos chunks recuperar:

```python
{
    "top_k": 5,              # Recuperar los 5 chunks m√°s similares
    "score_threshold": 0.7   # Que tengan ‚â• 0.7 de similitud
}
```

**Configuraci√≥n recomendada:**

| Escenario | top_k | score_threshold |
|-----------|-------|-----------------|
| Respuestas precisas | 3-5 | 0.8-0.9 |
| **Uso general** ‚úÖ | 5-7 | 0.7-0.8 |
| Respuestas detalladas | 10-15 | 0.6-0.7 |
| Exploraci√≥n | 15-20 | 0.5-0.6 |

---

## üóÑÔ∏è Vector Store (Qdrant)

### ¬øQu√© es?

Qdrant es una base de datos especializada en almacenar y buscar vectores de alta dimensionalidad de manera eficiente.

### Estructura de datos

```python
Collection: "tenant_517d45e0-f975-4bb9-84fb-232c33f6e6dd"
‚îú‚îÄ‚îÄ Vector 1 (UUID: abc123)
‚îÇ   ‚îú‚îÄ‚îÄ vector: [0.2, 0.8, ..., 0.5]  # 1024 dimensiones
‚îÇ   ‚îî‚îÄ‚îÄ payload: {
‚îÇ       "content": "Texto del chunk...",
‚îÇ       "document_id": "doc-uuid",
‚îÇ       "chunk_index": 0,
‚îÇ       "tenant_id": "tenant-uuid",
‚îÇ       "metadata": {...}
‚îÇ   }
‚îú‚îÄ‚îÄ Vector 2 (UUID: def456)
‚îÇ   ‚îî‚îÄ‚îÄ ...
```

### Collections por tenant

Cada tenant tiene su propia collection aislada:

```
tenant_517d45e0-f975-4bb9-84fb-232c33f6e6dd  (Usuario 1)
tenant_8a2f3b1c-4d5e-6f7g-8h9i-0j1k2l3m4n5o  (Usuario 2)
```

**Ventajas:**
- **Aislamiento**: Un usuario no puede acceder a datos de otro
- **Escalabilidad**: Collections independientes
- **Rendimiento**: B√∫squedas solo en datos del usuario

### Configuraci√≥n

```bash
QDRANT_URL=http://qdrant:6333
QDRANT_TIMEOUT=30              # Timeout de operaciones
QDRANT_PREFER_GRPC=False       # Usar HTTP en lugar de gRPC
```

---

## üîÑ RAG Pipeline (Flujo Completo)

### Proceso de Indexaci√≥n (Upload Document)

```
1. Upload
   ‚Üì
   üìÑ documento.pdf (50 p√°ginas)

2. Parsing (DocumentLoader)
   ‚Üì
   üìù Texto extra√≠do (25,000 caracteres)

3. Chunking (TextChunker)
   ‚Üì
   üìã 50 chunks √ó 500 chars cada uno

4. Embedding (BAAI/bge-m3)
   ‚Üì
   üß† 50 vectores √ó 1024 dimensiones

5. Storage (Qdrant)
   ‚Üì
   üóÑÔ∏è Collection: tenant_xxx
       ‚îî‚îÄ‚îÄ 50 vectors con metadata
```

### Proceso de Query

```
1. Usuario hace pregunta
   ‚Üì
   "¬øC√≥mo resetear la contrase√±a?"

2. Generar embedding de la query
   ‚Üì
   üß† [0.3, 0.7, 0.2, ..., 0.5]

3. Similarity Search en Qdrant
   ‚Üì
   üîç Buscar top_k=5 chunks m√°s similares
       con score >= 0.7

4. Recuperar contextos
   ‚Üì
   üìã 5 chunks relevantes:
       - Chunk 23: score=0.92
       - Chunk 45: score=0.88
       - Chunk 12: score=0.85
       - Chunk 67: score=0.78
       - Chunk 34: score=0.72

5. Construir prompt para LLM
   ‚Üì
   üìù Contexto + Pregunta

6. LLM genera respuesta
   ‚Üì
   ü§ñ Ollama (phi3:mini)

7. Respuesta final
   ‚Üì
   ‚úÖ "Para resetear tu contrase√±a..."
```

---

## ‚öôÔ∏è Par√°metros de Query Explicados

### Tabla completa de par√°metros

| Par√°metro | Tipo | Rango | Default | Descripci√≥n |
|-----------|------|-------|---------|-------------|
| `query` | string | 1-1000 | - | **Requerido**: Tu pregunta |
| `top_k` | int | 1-20 | 5 | N√∫mero de chunks a recuperar |
| `score_threshold` | float | 0.0-1.0 | 0.7 | Umbral m√≠nimo de similitud |
| `temperature` | float | 0.0-2.0 | 0.7 | Creatividad del LLM |
| `max_tokens` | int | 1-4000 | None | L√≠mite de tokens en respuesta |
| `use_hybrid_search` | bool | true/false | false | Combinar b√∫squeda vectorial + keyword |

### Temperature (Creatividad del LLM)

Controla la aleatoriedad en las respuestas del LLM:

```python
temperature = 0.0   # Determinista, siempre la misma respuesta
              ‚Üì
temperature = 0.3   # Muy preciso, poco creativo
              ‚Üì
temperature = 0.7   # ‚úÖ Balanceado (recomendado)
              ‚Üì
temperature = 1.0   # Creativo, variado
              ‚Üì
temperature = 2.0   # Muy aleatorio, puede divagar
```

**Uso recomendado:**

| Caso de uso | Temperature |
|-------------|-------------|
| FAQ / Soporte t√©cnico | 0.3 - 0.5 |
| **Uso general** ‚úÖ | 0.7 - 0.9 |
| Brainstorming / Creatividad | 1.0 - 1.5 |
| Experimentaci√≥n | 1.5 - 2.0 |

### Max Tokens

Limita la longitud de la respuesta:

```python
max_tokens = 50    # Respuesta muy breve
max_tokens = 200   # Respuesta concisa ‚úÖ
max_tokens = 400   # Respuesta detallada
max_tokens = 1000  # Respuesta muy extensa
```

**Nota**: 1 token ‚âà 0.75 palabras en ingl√©s, ‚âà 0.5 palabras en espa√±ol

---

## üîÄ Hybrid Search (B√∫squeda H√≠brida)

### ¬øQu√© es?

Combina dos tipos de b√∫squeda:

1. **B√∫squeda vectorial (sem√°ntica)**: Por significado
2. **B√∫squeda por keywords (l√©xica)**: Por palabras exactas

### ¬øCu√°ndo usarla?

```python
# B√∫squeda vectorial (use_hybrid_search=false) ‚úÖ
Query: "¬øC√≥mo funciona el motor?"
Encuentra: "proceso de funcionamiento del motor", "operaci√≥n del sistema"

# B√∫squeda h√≠brida (use_hybrid_search=true)
Query: "error CODE_123"
Encuentra: Documentos con el c√≥digo exacto "CODE_123" + contexto sem√°ntico
```

**Usa hybrid search cuando:**
- Buscas c√≥digos de error espec√≠ficos
- Necesitas nombres propios exactos
- Quieres combinar precisi√≥n l√©xica con comprensi√≥n sem√°ntica

---

## üíæ Caching y Optimizaci√≥n

### Cache de Embeddings (Redis)

**Ubicaci√≥n**: `app/infrastructure/cache/cache_service.py`

```python
# Cache key format
embedding_cache:{hash_del_texto} ‚Üí vector_embedding

# TTL (Time To Live)
EMBEDDING_CACHE_TTL=2592000  # 30 d√≠as
```

**Beneficios:**
- **Performance**: Embeddings instant√°neos para textos repetidos
- **Costo**: No re-procesar el mismo texto
- **Escalabilidad**: Menos carga en el modelo

### Cache de Queries

```python
QUERY_CACHE_TTL=3600        # 1 hora
RESPONSE_CACHE_TTL=1800     # 30 minutos
```

**Funcionamiento:**

```
Query 1 (11:00): "¬øCu√°l es el precio?"
  ‚Üì Procesa completo
  ‚Üì Guarda en cache (1 hora)

Query 2 (11:15): "¬øCu√°l es el precio?"
  ‚Üì Usa cache (instant√°neo) ‚úÖ

Query 3 (12:30): "¬øCu√°l es el precio?"
  ‚Üì Cache expirado, procesa nuevamente
```

---

## üìä M√©tricas y Observabilidad

### Campos registrados en queries

Cada query guarda:

```python
{
    "query_text": "¬øC√≥mo resetear?",
    "answer_text": "Para resetear...",
    "model_used": "phi3:mini",
    "tokens_used": 234,
    "processing_time": 2.5,  # segundos
    "num_contexts": 5,
    "created_at": "2025-12-31T10:30:00"
}
```

### Endpoint de historial

```bash
GET /api/v1/query/history?limit=50&offset=0
```

√ötil para:
- Analizar patrones de uso
- Mejorar respuestas frecuentes
- Optimizar configuraci√≥n (top_k, threshold)

---

## üéØ Best Practices

### 1. Configuraci√≥n de Chunking

```python
# ‚úÖ BIEN
DEFAULT_CHUNK_SIZE=500
DEFAULT_CHUNK_OVERLAP=50

# ‚ùå MAL - Chunks muy grandes
DEFAULT_CHUNK_SIZE=2000  # Pierde precisi√≥n en b√∫squeda

# ‚ùå MAL - Sin overlap
DEFAULT_CHUNK_OVERLAP=0  # Pierde contexto en l√≠mites
```

### 2. Ajuste de Score Threshold

```python
# Si obtienes muy pocos resultados
score_threshold=0.7  ‚Üí Reducir a 0.6

# Si obtienes resultados irrelevantes
score_threshold=0.7  ‚Üí Aumentar a 0.8
```

### 3. Balance Top K vs Score Threshold

```python
# ‚úÖ BIEN - Balance
top_k=5, score_threshold=0.7

# ‚ö†Ô∏è CUIDADO - Puede no devolver nada
top_k=20, score_threshold=0.95  # Muy estricto

# ‚ö†Ô∏è CUIDADO - Muchos irrelevantes
top_k=20, score_threshold=0.4   # Muy permisivo
```

### 4. Modelos de Embeddings

```python
# ‚úÖ Producci√≥n - Multiling√ºe
EMBEDDING_MODEL=BAAI/bge-m3

# Alternativas:
# - BAAI/bge-large-en-v1.5 (solo ingl√©s, mayor precisi√≥n)
# - sentence-transformers/all-MiniLM-L6-v2 (m√°s r√°pido, menor precisi√≥n)
# - intfloat/multilingual-e5-large (excelente multiling√ºe)
```

---

## üîß Troubleshooting

### Problema: Respuestas no relevantes

**Soluci√≥n:**
1. Aumentar `score_threshold` (0.7 ‚Üí 0.8)
2. Revisar calidad de los chunks
3. Verificar que el documento fue indexado correctamente

### Problema: No encuentra informaci√≥n que s√≠ existe

**Soluci√≥n:**
1. Reducir `score_threshold` (0.7 ‚Üí 0.6)
2. Aumentar `top_k` (5 ‚Üí 10)
3. Usar `use_hybrid_search=true`

### Problema: Respuestas muy largas o cortas

**Soluci√≥n:**
1. Ajustar `max_tokens` (100-500 para respuestas normales)
2. Modificar `temperature` para controlar creatividad

### Problema: B√∫squedas lentas

**Soluci√≥n:**
1. Verificar cache de Redis est√° funcionando
2. Reducir `top_k` si es muy alto
3. Verificar tama√±o de la collection en Qdrant

---

## üìö Referencias

### Archivos clave del proyecto

- `app/rag/ingestion/chunker.py` - L√≥gica de chunking
- `app/rag/embeddings/sentence_transformers_provider.py` - Generaci√≥n de embeddings
- `app/rag/retrieval/vector_retriever.py` - Similarity search
- `app/rag/pipeline.py` - Pipeline completo de RAG
- `app/schemas/query.py` - Schemas de queries

### Variables de entorno importantes

```bash
# Chunking
DEFAULT_CHUNK_SIZE=500
DEFAULT_CHUNK_OVERLAP=50

# Embeddings
EMBEDDING_MODEL=BAAI/bge-m3
EMBEDDING_DEVICE=cpu
EMBEDDING_BATCH_SIZE=32
EMBEDDING_MAX_LENGTH=512
EMBEDDING_CACHE_TTL=2592000

# RAG
DEFAULT_TOP_K=5
DEFAULT_RETRIEVAL_SCORE_THRESHOLD=0.7
CONTEXT_MAX_TOKENS=2000

# Cache
QUERY_CACHE_TTL=3600
RESPONSE_CACHE_TTL=1800
```

### Recursos externos

- [BAAI/bge-m3 Model Card](https://huggingface.co/BAAI/bge-m3)
- [Qdrant Documentation](https://qdrant.tech/documentation/)
- [Sentence Transformers](https://www.sbert.net/)
- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)

---

## üí° Resumen R√°pido

| Concepto | Qu√© hace | Par√°metro clave | Valor recomendado |
|----------|----------|-----------------|-------------------|
| **Chunking** | Divide documentos | `CHUNK_SIZE` | 500 chars |
| **Embeddings** | Convierte texto a vectores | `EMBEDDING_MODEL` | BAAI/bge-m3 |
| **Similarity Search** | Encuentra chunks relevantes | `score_threshold` | 0.7 |
| **Top K** | L√≠mite de resultados | `top_k` | 5 |
| **Temperature** | Creatividad del LLM | `temperature` | 0.7 |
| **Max Tokens** | Longitud de respuesta | `max_tokens` | 200-400 |
